# -*- coding: utf-8 -*-
"""distillGenerator.ipynb

Automatically generated by Colaboratory.
"""

from google.colab import drive

drive.mount('/content/drive')
file_path = #

# Adjusting the paths for saving the model and the loss list
model_save_path = f"{file_path}/333student_model_05"
loss_list_save_path = f"{file_path}/333loss_list_05.pt"

!pip install --quiet datasets
!pip install --quiet transformers datasets torch
!pip install --quiet transformers[torch]
!pip install --quiet accelerate -U
!pip install --quiet tqdm

from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import numpy as np
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

teacher_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
teacher_model = teacher_model.to(device)

student_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
student_model = student_model.to(device)

# it turns out both "google/flan-t5-large" and "google/flan-t5-small" use the same tokenizer
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")





teacher_model.to("cpu")
# Apply dynamic quantization (note: T5 may have limited support for dynamic quantization beyond Linear layers)
quantized_model = torch.quantization.quantize_dynamic(
    teacher_model, {torch.nn.Linear}, dtype=torch.qint8
)

teacher_model.to(device)

# Prepare and tokenize the dataset
def tokenize_and_format(examples):
    # Tokenizing the input texts
    tokenized_inputs = tokenizer(examples['input_text'], padding="max_length", truncation=True, max_length=512)
    # Tokenizing the target texts
    tokenized_targets = tokenizer(examples['target_text'], padding="max_length", truncation=True, max_length=512)

    return {
        'input_ids': tokenized_inputs.input_ids,
        'attention_mask': tokenized_inputs.attention_mask,
        'labels': tokenized_targets.input_ids
    }

# Load and preprocess dataset
raw_dataset = load_dataset("Nicolas-BZRD/Parallel_Global_Voices_English_French", split="train")
raw_dataset = raw_dataset.select(range(10000))
processed_dataset = raw_dataset.map(lambda examples: {'input_text': ["translate English to French: " + ex for ex in examples["en"]],
                                                      'target_text': examples["fr"]},
                                    batched=True)
tokenized_dataset = processed_dataset.map(tokenize_and_format, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Prepare DataLoader
train_dataloader = DataLoader(tokenized_dataset, batch_size=6, shuffle=True)

from torch.nn.functional import kl_div, log_softmax, cross_entropy
from torch.nn import KLDivLoss
import torch.nn.functional as F

def kl_divergence_loss(student_logits, teacher_logits, temperature):
    """
    Compute the KL divergence loss to measure how closely the student's predictions match the teacher's.
    """
    # Soften the logits and convert teacher logits into probabilities
    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)
    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
    # Compute KL divergence. Note: the target (teacher_probs) is not in log space here.
    kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean', log_target=False) * (temperature ** 2)
    return kl_loss

def ground_truth_loss(student_logits, labels):
    """
    Compute the cross-entropy loss between the student's predictions and the true labels,
    ensuring correct dimension alignment.
    """
    return F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))

def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.9):
    """
    Compute the final distillation loss as a weighted average of KL divergence and cross-entropy loss.
    :param student_logits: Logits from the student model.
    :param teacher_logits: Logits from the teacher model.
    :param labels: Ground truth labels.
    :param temperature: Temperature for softening probabilities. A higher value produces softer probabilities.
    :param alpha: Weighting factor for the loss components. 0 <= alpha <= 1.
    """
    # Compute the KL divergence loss (student vs. teacher)
    distill_loss = kl_divergence_loss(student_logits, teacher_logits, temperature)

    # Compute the ground truth loss (student vs. true labels)
    gt_loss = ground_truth_loss(student_logits, labels)

    # Combine the losses
    total_loss = alpha * distill_loss + (1 - alpha) * gt_loss
    return total_loss

def shift_labels_to_decoder_input_ids(labels, pad_token_id):
    """
    Shift labels to the right to create decoder_input_ids.

    Args:
        labels (torch.Tensor): Tensor of labels of shape [batch_size, sequence_length].
        pad_token_id (int): The pad token id to use for shifting.

    Returns:
        torch.Tensor: Tensor of shifted labels suitable for decoder input.
    """
    # Create a tensor of the same shape as labels filled with pad_token_id
    pad_tensor = torch.full_like(labels, pad_token_id)

    # Shift labels to the right by slicing and concatenating pad_tensor at the beginning
    decoder_input_ids = torch.cat([pad_tensor[:, :1], labels[:, :-1]], dim=-1)

    return decoder_input_ids

from torch.optim import AdamW
from torch.nn import CrossEntropyLoss

optimizer = AdamW(student_model.parameters(), lr=5e-5)

# prompt: only train the last few layers of student model

#for module in student_model.parameters():
    #module.requires_grad = False

#for module in student_model.decoder.layers[-3:]:
    #module.requires_grad = True

epochs = 3
loss_list = []

for epoch in range(epochs):
    student_model.train()
    total_loss = 0

    for batch in tqdm(train_dataloader, desc=f"Training Epoch {epoch + 1}"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass for student model
        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        student_logits = outputs.logits.to(device)

        decoder_input_ids = shift_labels_to_decoder_input_ids(labels, teacher_model.config.pad_token_id)

        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)
            teacher_logits = teacher_outputs.logits.to(device)

        # Calculate distillation loss
        loss = distillation_loss(student_logits, teacher_logits, labels, temperature=1.25, alpha=0.5)

        if torch.isnan(loss):
            raise ValueError("NaN loss detected")

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    loss_list.append(total_loss / len(train_dataloader))
    print(f"Epoch {epoch + 1}: Average Loss = {total_loss / len(train_dataloader)}")

# Saving the model and the loss list in the specified directory
student_model.save_pretrained(model_save_path)
torch.save(loss_list, loss_list_save_path)

print(f"Model saved to {model_save_path}")
print(f"Loss list saved to {loss_list_save_path}")

student_model_05 = AutoModelForSeq2SeqLM.from_pretrained(model_save_path)
loss_list_05 = torch.load(loss_list_save_path)

print(student_model_05)
print(loss_list_05)

import matplotlib.pyplot as plt

# Plotting the combined loss curve for all batches across all epochs
plt.figure(figsize=(12, 8))
plt.plot(loss_list, label='Loss per Epochs', marker='o', linestyle='-')
plt.title('Loss Curve Across All Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
print(loss_list)

"""### Evaluation"""

base_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
base_model.to(device)

# Prepare DataLoader
eval_dataset = load_dataset("Nicolas-BZRD/Parallel_Global_Voices_English_French", split="train")

eval_dataset = eval_dataset.select(range(10000,12000))
eval_dataset = eval_dataset.map(lambda example: {'input_text': ["translate English to French: " + ex for ex in example["en"]],
                                                  'target_text': example["fr"]},
                                batched=True)
eval_dataset = eval_dataset.map(tokenize_and_format, batched=True)

eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
eval_dataloader = DataLoader(eval_dataset, batch_size=4)

from tqdm.auto import tqdm

teacher_predictions, student_predictions, base_predictions = [], [], []

# Assuming base_model is also loaded and set to the correct device
base_model.eval()
student_model.eval()
teacher_model.eval()

# Loop for teacher, student, and base models (GPU compatible)
for batch in tqdm(eval_dataloader, desc="Generating predictions for Teacher, Student, and Base models"):
    input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)

    with torch.no_grad():
        # Teacher model predictions
        teacher_outputs = teacher_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Student model predictions
        student_outputs = student_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Base model predictions
        base_outputs = base_model.generate(input_ids=input_ids, attention_mask=attention_mask)

    # Decode the generated ids to text for all models
    teacher_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in teacher_outputs]
    student_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in student_outputs]
    base_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in base_outputs]

    teacher_predictions.extend(teacher_preds)
    student_predictions.extend(student_preds)
    base_predictions.extend(base_preds)

from datasets import load_metric

# Initialize the BLEU metric
bleu_metric = load_metric("bleu")

# Prepare references: Each teacher prediction needs to be a list of lists
references = [[ref.split()] for ref in teacher_predictions]  # Teacher predictions as reference

# Prepare candidates: Student and base model predictions
student_candidates = [pred.split() for pred in student_predictions]
base_candidates = [pred.split() for pred in base_predictions]

# Compute BLEU score for the student model
student_bleu = bleu_metric.compute(predictions=student_candidates, references=references)
student_bleu_score = student_bleu['bleu']

# Compute BLEU score for the base model
base_bleu = bleu_metric.compute(predictions=base_candidates, references=references)
base_bleu_score = base_bleu['bleu']

# Display the results
print(f"Student Model BLEU Score: {student_bleu_score:.4f}")
print(f"Base Model BLEU Score: {base_bleu_score:.4f}")

quant_predictions = []

quantized_model.to('cpu')
quantized_model.eval()

# Loop for the quantized model (CPU execution)
for batch in tqdm(eval_dataloader, desc="Generating predictions for Quantized model"):
    input_ids, attention_mask = batch['input_ids'].to('cpu'), batch['attention_mask'].to('cpu')

    with torch.no_grad():
        # Quantized model predictions
        quant_outputs = quantized_model.generate(input_ids=input_ids, attention_mask=attention_mask)

    quant_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in quant_outputs]

    quant_predictions.extend(quant_preds)

# Prepare candidates: Student and base model predictions
quant_candidates = [pred.split() for pred in quant_predictions]

quant_bleu = bleu_metric.compute(predictions=quant_candidates, references=references)
quant_bleu_score = quant_bleu['bleu']

# Display the results
print(f"Student Model BLEU Score: {student_bleu_score:.4f}")
print(f"Base Model BLEU Score: {base_bleu_score:.4f}")
print(f"Quantized Model BLEU Score: {quant_bleu_score:.4f}")

# Specify the directory path
file_path = '/content/drive/My Drive/cse417/projectFinal'

# Define full file paths for saving predictions
teacher_predictions_file = f'{file_path}/teacher_predictions.txt'
student_predictions_file = f'{file_path}/student_predictions05.txt'
base_predictions_file = f'{file_path}/base_predictions.txt'
quant_predictions_file = f'{file_path}/quant_predictions.txt'  # For quantized model predictions

def save_predictions(predictions, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        for prediction in predictions:
            f.write(prediction + '\n')

save_predictions(teacher_predictions, teacher_predictions_file)
save_predictions(student_predictions, student_predictions_file)
save_predictions(base_predictions, base_predictions_file)
save_predictions(quant_predictions, quant_predictions_file)

#load this models

# Define the paths to the saved prediction files
teacher_predictions_file = '/content/drive/My Drive/cse417/projectFinal/teacher_predictions.txt'
student_predictions_file = '/content/drive/My Drive/cse417/projectFinal/student_predictions.txt'
base_predictions_file = '/content/drive/My Drive/cse417/projectFinal/base_predictions.txt'
quant_predictions_file = '/content/drive/My Drive/cse417/projectFinal/quant_predictions.txt'

# Function to load predictions from a file
def load_predictions(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f]

# Load predictions for all models
teacher_predictions = load_predictions(teacher_predictions_file)
student_predictions = load_predictions(student_predictions_file)
base_predictions = load_predictions(base_predictions_file)
quant_predictions = load_predictions(quant_predictions_file)

# Model names and their corresponding BLEU scores
models = ['Base Model (Control)', 'Student Model (alpha=0.5)', 'Quantized Model']
bleu_scores = [base_bleu_score, student_bleu_score, quant_bleu_score]

clist = ['#1f77b4', '#60a3d9', '#a3d2f5']#,'#3399ff']

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(models, bleu_scores, color=clist)
plt.ylabel('BLEU Score')
plt.title('Comparison of BLEU Scores in English-to-French Translation')
plt.ylim(0, 0.5)  # BLEU scores range from 0 to 1


# Display the plot
plt.show()

#bleu_scores = [base_bleu_score, student_bleu_score, quant_bleu_score]
print(bleu_scores)

# calculate test accuracy/loss for student and teacher model
def test_model(model, dataloader, quant=False):
    if quant:
      model = model.cpu()
      device = 'cpu'
    else:
      device= 'cuda'
      model = model.to(device)
    model.eval()
    total_loss = 0
    total_correct = 0
    total_count = 0
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Testing"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            logits = outputs.logits

            total_loss += outputs.loss.item()

            # Calculate accuracy
            predictions = torch.argmax(logits, dim=-1)
            correct = (predictions == labels).sum().item()
            total_correct += correct
            total_count += labels.numel()

    model = model.cpu()
    device = 'cuda'

    return total_loss / len(dataloader), total_correct / total_count

teacher_loss, teacher_accuracy = test_model(teacher_model, eval_dataloader)

base_loss, base_accuracy = test_model(base_model, eval_dataloader)

student05_loss, student05_accuracy = test_model(student_model, eval_dataloader)

quantized_loss, quantized_accuracy = test_model(quantized_model, eval_dataloader, quant=True)

print(f"Teacher Eval Loss: {teacher_loss:.4f}, Teacher Eval Accuracy: {teacher_accuracy:.4f}")
print(f"Base Eval Loss: {base_loss:.4f}, Student Eval Accuracy: {base_accuracy:.4f}")
print(f"Student with alpha=0.5 Eval Loss: {student05_loss:.4f}, Student Eval Accuracy: {student05_accuracy:.4f}")
print(f"Quantized Eval Loss: {quantized_loss:.4f}, Quantized Eval Accuracy: {quantized_accuracy:.4f}")

# Model names and their corresponding BLEU scores
models = ['Base Model (Control)', 'Student Model (alpha=0.5)', 'Quantized Model']
loss_scores =  torch.tensor([base_accuracy, student05_accuracy, quantized_accuracy]) / teacher_accuracy


Llist = ['#1f77b4', '#60a3d9', '#a3d2f5']#, '#4e79a7'] #'#93c7e4', '#b5cfe7']

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(models, loss_scores, color=Llist)
plt.ylabel("Loss Percentage Relative to the Teacher Model")
plt.title("Comparison of Loss Scores in English-to-French Translation")

plt.legend()

# Display the plot
plt.show()

#models = ['Base Model (Control)', 'Student Model (alpha=0.5)', 'Quantized Model']
#loss_scores =  torch.tensor([base_accuracy, student05_accuracy, quantized_accuracy]) / teacher_accuracy
print(loss_scores)

# Prepare and tokenize the dataset
def tokenize_and_format_korean(examples):
    # Tokenizing the input texts
    tokenized_inputs = tokenizer(examples['input_text'], padding="max_length", truncation=True, max_length=512)
    # Tokenizing the target texts
    tokenized_targets = tokenizer(examples['target_text'], padding="max_length", truncation=True, max_length=512)

    return {
        'input_ids': tokenized_inputs.input_ids,
        'attention_mask': tokenized_inputs.attention_mask,
        'labels': tokenized_targets.input_ids
    }

# Load and preprocess dataset
raw_korean_dataset = load_dataset("msarmi9/korean-english-multitarget-ted-talks-task", split="test")
raw_korean_dataset = raw_korean_dataset.select(range(1000))
processed_korean_dataset = raw_korean_dataset.map(lambda examples: {'input_text': ["translate Korean to English: " + ex for ex in examples["korean"]],
                                                      'target_text': examples["english"]},
                                    batched=True)
tokenized_korean_dataset = processed_korean_dataset.map(tokenize_and_format_korean, batched=True)
tokenized_korean_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Prepare DataLoader
korean_eval_dataloader = DataLoader(tokenized_korean_dataset, batch_size=16)

from tqdm.auto import tqdm

teacher_predictions_korean, student_predictions_korean, base_predictions_korean = [], [], []

base_model.to(device)
student_model.to(device)
teacher_model.to(device)

# Assuming base_model is also loaded and set to the correct device
base_model.eval()
student_model.eval()
teacher_model.eval()

# Loop for teacher, student, and base models (GPU compatible)
for batch in tqdm(korean_eval_dataloader, desc="Generating predictions for Teacher, Student, and Base models"):
    input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)

    with torch.no_grad():
        # Teacher model predictions
        teacher_outputs = teacher_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Student model predictions
        student_outputs = student_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Base model predictions
        base_outputs = base_model.generate(input_ids=input_ids, attention_mask=attention_mask)

    # Decode the generated ids to text for all models
    teacher_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in teacher_outputs]
    student_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in student_outputs]
    base_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in base_outputs]

    teacher_predictions_korean.extend(teacher_preds)
    student_predictions_korean.extend(student_preds)
    base_predictions_korean.extend(base_preds)

from tqdm.auto import tqdm

teacher_predictions_korean, student_predictions_korean, base_predictions_korean = [], [], []

base_model.to(device)
student_model.to(device)
teacher_model.to(device)

# Assuming base_model is also loaded and set to the correct device
base_model.eval()
student_model.eval()
teacher_model.eval()

# Loop for teacher, student, and base models (GPU compatible)
for batch in tqdm(korean_eval_dataloader, desc="Generating predictions for Teacher, Student, and Base models"):
    input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)

    with torch.no_grad():
        # Teacher model predictions
        teacher_outputs = teacher_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Student model predictions
        student_outputs = student_model.generate(input_ids=input_ids, attention_mask=attention_mask)
        # Base model predictions
        #base_outputs = base_model.generate(input_ids=input_ids, attention_mask=attention_mask)


    # Decode the generated ids to text for all models
    teacher_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in teacher_outputs]
    #student_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in student_outputs]

    print(teacher_preds)
    #base_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in base_outputs]

    #teacher_predictions_korean.extend(teacher_preds)
    student_predictions_korean.extend(student_preds)
    #base_predictions_korean.extend(base_preds)

prompt = "Translate French to English : comment allez-vous? je vais bien?"
device1 = 'cpu'

# Tokenize the input and extract input_ids and attention_mask, ensuring they are on the same device as the model
tokenized_inputs = tokenizer(prompt, return_tensors="pt").to(device1)

input_ids = tokenized_inputs.input_ids.to(device1)
attention_mask = tokenized_inputs.attention_mask.to(device1)

# Generate the translation using the model
outputs = quantized_model.generate(input_ids, attention_mask=attention_mask)

# Decode the generated translation
translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(translation)

quant_predictions_korean = []

quantized_model.to('cpu')
quantized_model.eval()

# Loop for the quantized model (CPU execution)
for batch in tqdm(korean_eval_dataloader, desc="Generating predictions for Quantized model"):
    input_ids, attention_mask = batch['input_ids'].to('cpu'), batch['attention_mask'].to('cpu')

    with torch.no_grad():
        # Quantized model predictions
        quant_outputs = quantized_model.generate(input_ids=input_ids, attention_mask=attention_mask)

    quant_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in quant_outputs]

    quant_predictions_korean.extend(quant_preds)

quant_predictions_korean = []

quantized_model.to('cpu')
quantized_model.eval()

# Loop for the quantized model (CPU execution)
for batch in tqdm(korean_eval_dataloader, desc="Generating predictions for Quantized model"):
    input_ids, attention_mask = batch['input_ids'].to('cpu'), batch['attention_mask'].to('cpu')

    with torch.no_grad():
        # Quantized model predictions
        quant_outputs = quantized_model.generate(input_ids=input_ids, attention_mask=attention_mask)

    quant_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in quant_outputs]

    quant_predictions_korean.extend(quant_preds)


# Specify the directory path
file_path = '/content/drive/My Drive/cse417/projectFinal'

# Define full file paths for saving predictions
teacher_predictions_file = f'{file_path}/korean_teacher_predictions.txt'
student_predictions_file = f'{file_path}/korean_student_predictions05.txt'
base_predictions_file = f'{file_path}/korean_base_predictions.txt'
quant_predictions_file = f'{file_path}/korean_quant_predictions.txt'  # For quantized model predictions
save_predictions(teacher_predictions_korean, teacher_predictions_file)
save_predictions(student_predictions_korean, student_predictions_file)
save_predictions(base_predictions_korean, base_predictions_file)
save_predictions(quant_predictions_korean, quant_predictions_file)

save_predictions(teacher_predictions_korean, teacher_predictions_file)
save_predictions(student_predictions_korean, student_predictions_file)
save_predictions(base_predictions_korean, base_predictions_file)
save_predictions(quant_predictions_korean, quant_predictions_file)

from datasets import load_metric
bleu_metric = load_metric('bleu')

from datasets import load_metric
bleu_metric = load_metric('bleu')

# Prepare references: Each teacher prediction needs to be a list of lists of tokens
references_korean = [[ref.split()] for ref in teacher_predictions_korean]

# Prepare candidates: Tokenize the predictions from each model
student_candidates_korean = [pred.split() for pred in student_predictions_korean]
base_candidates_korean = [pred.split() for pred in base_predictions_korean]
quant_candidates_korean = [pred.split() for pred in quant_predictions_korean]  # Assuming this is defined

# Compute BLEU score for the student model
student_bleu_korean = bleu_metric.compute(predictions=student_candidates_korean, references=references_korean)
student_bleu_score_korean = student_bleu_korean['bleu']

# Compute BLEU score for the base model
base_bleu_korean = bleu_metric.compute(predictions=base_candidates_korean, references=references_korean)
base_bleu_score_korean = base_bleu_korean['bleu']

# Compute BLEU score for the quantized model
quant_bleu_korean = bleu_metric.compute(predictions=quant_candidates_korean, references=references_korean)
quant_bleu_score_korean = quant_bleu_korean['bleu']

# Display the results
print(f"Student Model BLEU Score: {student_bleu_score_korean:.4f}")
print(f"Base Model BLEU Score: {base_bleu_score_korean:.4f}")
print(f"Quantized Model BLEU Score: {quant_bleu_score_korean:.4f}")

import pprint
pprint.pprint(student_candidates_korean)

studentKorean_loss, studentKorean_accuracy = test_model(student_model, korean_eval_dataloader)

print(f"Student Eval Loss: {studentKorean_loss:.4f}, Student Eval Accuracy: {studentKorean_accuracy:.4f}")

teacherKorean_loss, teacherKorean_accuracy = test_model(teacher_model, korean_eval_dataloader)

print(f"Teacher Eval Loss: {teacherKorean_loss:.4f}, Teacher Eval Accuracy: {teacherKorean_accuracy:.4f}")

print(references_korean)

print(student_candidates_korean)







# Model names and their corresponding BLEU scores
models = ['Base Model (Control)', 'Student Model (alpha=0.5)', 'Quantized Model']
bleu_scores = [base_bleu_score, student_bleu_score, quant_bleu_score]

clist = ['#1f77b4', '#60a3d9', '#a3d2f5']#,'#3399ff']

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(models, bleu_scores, color=clist)
plt.ylabel('BLEU Score')
plt.title('Comparison of BLEU Scores in English-to-French Translation')
plt.ylim(0, 0.5)  # BLEU scores range from 0 to 1


# Display the plot
plt.show()









avg_kl_div_teacher, kl_div_list_teacher = model_kl_div(teacher_model, teacher_tokenizer, eval_dataloader)
avg_kl_div_student, kl_div_list_student = model_kl_div(student_model, student_tokenizer, eval_dataloader)
avg_kl_div_quantized, kl_div_list_quantized = model_kl_div(quantized_model, teacher_tokenizer, eval_dataloader)

print(f"Average KL Divergence Teacher: {avg_kl_div_teacher:.4f}")
print(f"Average KL Divergence Student: {avg_kl_div_student:.4f}")
print(f"Average KL Divergence Quantized: {avg_kl_div_quantized:.4f}")

#plot kl div list
plt.figure(figsize=(12, 8))
plt.plot(kl_div_list_teacher, label='Teacher', marker='o', linestyle='-')
plt.plot(kl_div_list_student, label='Student', marker='o', linestyle='-')
plt.plot(kl_div_list_quantized, label='Quantized', marker='o', linestyle='-')
plt.title('KL Divergence Comparison')
plt.xlabel('Data point')
plt.ylabel('KL Divergence')
plt.legend()
plt.grid(True)
plt.show()

# prompt: Run an evaluation of the three models on Korean to English huggingface dataset called msarmi9/korean-english-multitarget-ted-talks-task. Plot the results

import matplotlib.pyplot as plt


# Evaluate models on Korean dataset
teacher_loss, teacher_accuracy = test_model(teacher_model, korean_eval_dataloader)
student_loss, student_accuracy = test_model(student_model, korean_eval_dataloader)
quantized_loss, quantized_accuracy = test_model(quantized_model, korean_eval_dataloader, quant=True)

print(f"Teacher Eval Loss: {teacher_loss:.4f}, Teacher Eval Accuracy: {teacher_accuracy:.4f}")
print(f"Student Eval Loss: {student_loss:.4f}, Student Eval Accuracy: {student_accuracy:.4f}")
print(f"Quantized Eval Loss: {quantized_loss:.4f}, Quantized Eval Accuracy: {quantized_accuracy:.4f}")

# Plot the results
plt.figure(figsize=(12, 8))
plt.bar(["Teacher", "Student", "Quantized"], [teacher_accuracy, student_accuracy, quantized_accuracy], label="Accuracy")
plt.title("Accuracy Comparison on Korean Dataset")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

